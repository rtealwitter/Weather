{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlhhRswGx2Es"
      },
      "source": [
        "## Single Shot Detector for ExtremeWeather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXRh1Oix2Ew"
      },
      "source": [
        "### Preprocessing the Data\n",
        "The [ExtremeWeather](https://extremeweatherdataset.github.io/) data set consists of weather data for a specific 25km region from 1979 to 2005. \n",
        "The data is organized by year and contains 4 images per day. Each image has 768x1152 pixels across 16 channels for different weather variables. \n",
        "In addition, each day has up to 15 bounding boxes\n",
        "surrounding extreme weather events classified as Tropical Depression, Tropical Cyclone, Extratropical Cyclone, and Atmospheric River.\n",
        "\n",
        "The goal is to correctly identify and classify extreme weather events. \n",
        "The first challenge is the sheer size of the data set.\n",
        "Each year, even when compressed, takes 64 GB to store.\n",
        "As the \"small\" data set, we consider training on 1979 and 1981 then testing on 1984.\n",
        "We preprocess each year by considering only the month of July and rescaling each image so its of size 300x300.\n",
        "In addition, we choose 3 of the 16 channels by hand for 'visual explainability' of bounding boxes.\n",
        "All together, we are able to reduce the size of the compressed data from 64 GB to 450 MB consisting of 124 300x300 images and bounding boxes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV2oYcLXx2Ex"
      },
      "source": [
        "### Training the SSD Model\n",
        "Instead of building the SSD model from scratch, we use a pretrained model on the COCO database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0tLUa74x2Ey"
      },
      "source": [
        "## Import libraries\n",
        "import torch\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3A7BoQvx2Ez"
      },
      "source": [
        "gc.collect()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuMLngwx2Ez"
      },
      "source": [
        "from preprocess import read_data\n",
        "year = '1979'\n",
        "images, boxes = read_data(year)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}