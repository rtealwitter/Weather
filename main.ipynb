{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlhhRswGx2Es"
      },
      "source": [
        "## Single Shot Detector for ExtremeWeather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXRh1Oix2Ew"
      },
      "source": [
        "### Preprocessing the Data\n",
        "The [ExtremeWeather](https://extremeweatherdataset.github.io/) data set consists of weather data for a specific 25km region from 1979 to 2005. \n",
        "The data is organized by year and contains 4 images per day. Each image has 768x1152 pixels across 16 channels for different weather variables. \n",
        "In addition, each day has up to 15 bounding boxes\n",
        "surrounding extreme weather events classified as Tropical Depression, Tropical Cyclone, Extratropical Cyclone, and Atmospheric River.\n",
        "\n",
        "The goal is to correctly identify and classify extreme weather events. \n",
        "The first challenge is the sheer size of the data set.\n",
        "Each year, even when compressed, takes 64 GB to store.\n",
        "As the \"small\" data set, we consider training on 1979 and 1981 then testing on 1984.\n",
        "We preprocess each year by considering only the month of July and rescaling each image so its of size 300x300.\n",
        "In addition, we choose 3 of the 16 channels by hand for 'visual explainability' of bounding boxes.\n",
        "All together, we are able to reduce the size of the compressed data from 64 GB to 4 MB consisting of 124 300x300 images and bounding boxes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHqa8ulBK3OI"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgod1dxcyelL",
        "outputId": "a0339ad0-4512-4aab-cca9-8db2117b1798"
      },
      "source": [
        "## Load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leBCbKBjK1PD"
      },
      "source": [
        "## Set up\n",
        "### Loading Data\n",
        "import random\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "### Visualization\n",
        "from matplotlib import patches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### Loss function\n",
        "import itertools\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "### Model\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "### Training\n",
        "import time\n",
        "\n",
        "#### Ignore warnings\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bedVLEYwyX6A"
      },
      "source": [
        "### Box Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3cgLfHGiuMJ"
      },
      "source": [
        "One of the key insights of SSD is combining the predicted bounding boxes with the convolutional layers. The first step is to represent the ground truth bounding boxes in the higher dimensional space of the predicted bounding boxes. In particular, we consider 8732 possible bounding box positions in an image (at different aspect ratios, centers, and scales). Therefore we need to find the bounding boxes in this higher dimensional space most related to the ground truth bounding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn-qpWAdyXTJ"
      },
      "source": [
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/utils/meshgrid.py\n",
        "\n",
        "def meshgrid(x, y, row_major=True):\n",
        "    '''Return meshgrid in range x & y.\n",
        "    Args:\n",
        "      x: (int) first dim range.\n",
        "      y: (int) second dim range.\n",
        "      row_major: (bool) row major or column major.\n",
        "    Returns:\n",
        "      (tensor) meshgrid, sized [x*y,2]\n",
        "    '''\n",
        "    a = torch.arange(0,x)\n",
        "    b = torch.arange(0,y)\n",
        "    xx = a.repeat(y).view(-1,1)\n",
        "    yy = b.view(-1,1).repeat(1,x).view(-1,1)\n",
        "    return torch.cat([xx,yy],1) if row_major else torch.cat([yy,xx],1)\n",
        "\n",
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/utils/box.py\n",
        "def change_box_order(boxes, order):\n",
        "    '''Change box order between (xmin,ymin,xmax,ymax) and (xcenter,ycenter,width,height).\n",
        "    Args:\n",
        "      boxes: (tensor) bounding boxes, sized [N,4].\n",
        "      order: (str) either 'xyxy2xywh' or 'xywh2xyxy'.\n",
        "    Returns:\n",
        "      (tensor) converted bounding boxes, sized [N,4].\n",
        "    '''\n",
        "    assert order in ['xyxy2xywh','xywh2xyxy']\n",
        "    a = boxes[:,:2]\n",
        "    b = boxes[:,2:]\n",
        "    if order == 'xyxy2xywh':\n",
        "        return torch.cat([(a+b)/2,b-a], 1)\n",
        "    return torch.cat([a-b/2,a+b/2], 1)\n",
        "\n",
        "def box_clamp(boxes, xmin, ymin, xmax, ymax):\n",
        "    '''Clamp boxes.\n",
        "    Args:\n",
        "      boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax), sized [N,4].\n",
        "      xmin: (number) min value of x.\n",
        "      ymin: (number) min value of y.\n",
        "      xmax: (number) max value of x.\n",
        "      ymax: (number) max value of y.\n",
        "    Returns:\n",
        "      (tensor) clamped boxes.\n",
        "    '''\n",
        "    boxes[:,0].clamp_(min=xmin, max=xmax)\n",
        "    boxes[:,1].clamp_(min=ymin, max=ymax)\n",
        "    boxes[:,2].clamp_(min=xmin, max=xmax)\n",
        "    boxes[:,3].clamp_(min=ymin, max=ymax)\n",
        "    return boxes\n",
        "\n",
        "def box_select(boxes, xmin, ymin, xmax, ymax):\n",
        "    '''Select boxes in range (xmin,ymin,xmax,ymax).\n",
        "    Args:\n",
        "      boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax), sized [N,4].\n",
        "      xmin: (number) min value of x.\n",
        "      ymin: (number) min value of y.\n",
        "      xmax: (number) max value of x.\n",
        "      ymax: (number) max value of y.\n",
        "    Returns:\n",
        "      (tensor) selected boxes, sized [M,4].\n",
        "      (tensor) selected mask, sized [N,].\n",
        "    '''\n",
        "    mask = (boxes[:,0]>=xmin) & (boxes[:,1]>=ymin) \\\n",
        "         & (boxes[:,2]<=xmax) & (boxes[:,3]<=ymax)\n",
        "    boxes = boxes[mask,:]\n",
        "    return boxes, mask\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    '''Compute the intersection over union of two set of boxes.\n",
        "    The box order must be (xmin, ymin, xmax, ymax).\n",
        "    Args:\n",
        "      box1: (tensor) bounding boxes, sized [N,4].\n",
        "      box2: (tensor) bounding boxes, sized [M,4].\n",
        "    Return:\n",
        "      (tensor) iou, sized [N,M].\n",
        "    Reference:\n",
        "      https://github.com/chainer/chainercv/blob/master/chainercv/utils/bbox/bbox_iou.py\n",
        "    '''\n",
        "    N = box1.size(0)\n",
        "    M = box2.size(0)\n",
        "\n",
        "    lt = torch.max(box1[:,None,:2], box2[:,:2])  # [N,M,2]\n",
        "    rb = torch.min(box1[:,None,2:], box2[:,2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb-lt).clamp(min=0)      # [N,M,2]\n",
        "    inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n",
        "\n",
        "    area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\n",
        "    area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\n",
        "    iou = inter / (area1[:,None] + area2 - inter)\n",
        "    return iou\n",
        "\n",
        "def box_nms(bboxes, scores, threshold=0.5):\n",
        "    '''Non maximum suppression.\n",
        "    Args:\n",
        "      bboxes: (tensor) bounding boxes, sized [N,4].\n",
        "      scores: (tensor) confidence scores, sized [N,].\n",
        "      threshold: (float) overlap threshold.\n",
        "    Returns:\n",
        "      keep: (tensor) selected indices.\n",
        "    Reference:\n",
        "      https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/nms/py_cpu_nms.py\n",
        "    '''\n",
        "    x1 = bboxes[:,0]\n",
        "    y1 = bboxes[:,1]\n",
        "    x2 = bboxes[:,2]\n",
        "    y2 = bboxes[:,3]\n",
        "\n",
        "    areas = (x2-x1) * (y2-y1)\n",
        "    _, order = scores.sort(0, descending=True)\n",
        "\n",
        "    keep = []\n",
        "    while order.numel() > 0:\n",
        "        i = order[0].item() if order.size() != torch.Size([]) else order.item()\n",
        "        keep.append(i)\n",
        "\n",
        "        if order.numel() == 1:\n",
        "            break\n",
        "\n",
        "        xx1 = x1[order[1:]].clamp(min=x1[i].item())\n",
        "        yy1 = y1[order[1:]].clamp(min=y1[i].item())\n",
        "        xx2 = x2[order[1:]].clamp(max=x2[i].item())\n",
        "        yy2 = y2[order[1:]].clamp(max=y2[i].item())\n",
        "\n",
        "        w = (xx2-xx1).clamp(min=0)\n",
        "        h = (yy2-yy1).clamp(min=0)\n",
        "        inter = w * h\n",
        "\n",
        "        overlap = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        ids = (overlap<=threshold).nonzero().squeeze()\n",
        "        if ids.numel() == 0:\n",
        "            break\n",
        "        order = order[ids+1]\n",
        "    return torch.tensor(keep, dtype=torch.long) \n",
        "\n",
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/models/ssd/box_coder.py\n",
        "class SSDBoxCoder:\n",
        "    def __init__(self, ssd_model):\n",
        "        self.steps = ssd_model.steps\n",
        "        self.box_sizes = ssd_model.box_sizes\n",
        "        self.aspect_ratios = ssd_model.aspect_ratios\n",
        "        self.fm_sizes = ssd_model.fm_sizes\n",
        "        self.default_boxes = self._get_default_boxes()\n",
        "\n",
        "    def _get_default_boxes(self):\n",
        "        boxes = []\n",
        "        for i, fm_size in enumerate(self.fm_sizes):\n",
        "            for h, w in itertools.product(range(fm_size), repeat=2):\n",
        "                cx = (w + 0.5) * self.steps[i]\n",
        "                cy = (h + 0.5) * self.steps[i]\n",
        "\n",
        "                s = self.box_sizes[i]\n",
        "                boxes.append((cx, cy, s, s))\n",
        "\n",
        "                s = math.sqrt(self.box_sizes[i] * self.box_sizes[i+1])\n",
        "                boxes.append((cx, cy, s, s))\n",
        "\n",
        "                s = self.box_sizes[i]\n",
        "                for ar in self.aspect_ratios[i]:\n",
        "                    boxes.append((cx, cy, s * math.sqrt(ar), s / math.sqrt(ar)))\n",
        "                    boxes.append((cx, cy, s / math.sqrt(ar), s * math.sqrt(ar)))\n",
        "        return torch.Tensor(boxes)  # xywh\n",
        "\n",
        "    def encode(self, boxes, labels):\n",
        "        '''Encode target bounding boxes and class labels.\n",
        "        SSD coding rules:\n",
        "          tx = (x - anchor_x) / (variance[0]*anchor_w)\n",
        "          ty = (y - anchor_y) / (variance[0]*anchor_h)\n",
        "          tw = log(w / anchor_w) / variance[1]\n",
        "          th = log(h / anchor_h) / variance[1]\n",
        "        Args:\n",
        "          boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax), sized [#obj, 4].\n",
        "          labels: (tensor) object class labels, sized [#obj,].\n",
        "        Returns:\n",
        "          loc_targets: (tensor) encoded bounding boxes, sized [#anchors,4].\n",
        "          cls_targets: (tensor) encoded class labels, sized [#anchors,].\n",
        "        Reference:\n",
        "          https://github.com/chainer/chainercv/blob/master/chainercv/links/model/ssd/multibox_coder.py\n",
        "        '''\n",
        "        def argmax(x):\n",
        "            v, i = x.max(0)\n",
        "            j = v.max(0)[1].item()\n",
        "            return (i[j], j)\n",
        "\n",
        "        default_boxes = self.default_boxes  # xywh\n",
        "        default_boxes = change_box_order(default_boxes, 'xywh2xyxy')\n",
        "\n",
        "        ious = box_iou(default_boxes, boxes)  # [#anchors, #obj]\n",
        "        index = torch.LongTensor(len(default_boxes)).fill_(-1)\n",
        "        masked_ious = ious.clone()\n",
        "        while True:\n",
        "            i, j = argmax(masked_ious)\n",
        "            if masked_ious[i,j] < 1e-6:\n",
        "                break\n",
        "            index[i] = j\n",
        "            masked_ious[i,:] = 0\n",
        "            masked_ious[:,j] = 0\n",
        "\n",
        "        mask = (index<0) & (ious.max(1)[0]>=0.5)\n",
        "        if mask.any():\n",
        "            squeezed_nonzero = mask.nonzero().squeeze()\n",
        "            if torch.Size([]) == squeezed_nonzero.size():\n",
        "              squeezed_nonzero = torch.stack([squeezed_nonzero])\n",
        "            index[mask] = ious[squeezed_nonzero].max(1)[1]\n",
        "\n",
        "        boxes = boxes[index.clamp(min=0)]  # negative index not supported\n",
        "        boxes = change_box_order(boxes, 'xyxy2xywh')\n",
        "        default_boxes = change_box_order(default_boxes, 'xyxy2xywh')\n",
        "\n",
        "        variances = (0.1, 0.2)\n",
        "        loc_xy = (boxes[:,:2]-default_boxes[:,:2]) / default_boxes[:,2:] / variances[0]\n",
        "        loc_wh = torch.log(boxes[:,2:]/default_boxes[:,2:]) / variances[1]\n",
        "        loc_targets = torch.cat([loc_xy,loc_wh], 1)\n",
        "        cls_targets = 1 + labels[index.clamp(min=0)]\n",
        "        cls_targets[index<0] = 0\n",
        "        return loc_targets, cls_targets\n",
        "\n",
        "    def decode(self, loc_preds, cls_preds, score_thresh=0.6, nms_thresh=0.45):\n",
        "        '''Decode predicted loc/cls back to real box locations and class labels.\n",
        "        Args:\n",
        "          loc_preds: (tensor) predicted loc, sized [8732,4].\n",
        "          cls_preds: (tensor) predicted conf, sized [8732,21].\n",
        "          score_thresh: (float) threshold for object confidence score.\n",
        "          nms_thresh: (float) threshold for box nms.\n",
        "        Returns:\n",
        "          boxes: (tensor) bbox locations, sized [#obj,4].\n",
        "          labels: (tensor) class labels, sized [#obj,].\n",
        "        '''\n",
        "        variances = (0.1, 0.2)\n",
        "        xy = loc_preds[:,:2] * variances[0] * self.default_boxes[:,2:] + self.default_boxes[:,:2]\n",
        "        wh = torch.exp(loc_preds[:,2:]*variances[1]) * self.default_boxes[:,2:]\n",
        "        box_preds = torch.cat([xy-wh/2, xy+wh/2], 1)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        scores = []\n",
        "        num_classes = cls_preds.size(1)\n",
        "        for i in range(num_classes-1):\n",
        "            score = cls_preds[:,i+1]  # class i corresponds to (i+1) column\n",
        "            mask = score > score_thresh\n",
        "            if not mask.any():\n",
        "                continue\n",
        "            box = box_preds[mask.nonzero().squeeze()]\n",
        "            score = score[mask]\n",
        "            if len(box.shape) == 1:\n",
        "              box = torch.stack([box])\n",
        "            keep = box_nms(box, score, nms_thresh)\n",
        "            boxes.append(box[keep])\n",
        "            labels.append(torch.LongTensor(len(box[keep])).fill_(i))\n",
        "            scores.append(score[keep])\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "          return torch.Tensor(boxes), torch.Tensor(labels), torch.Tensor(scores)\n",
        "        boxes = torch.cat(boxes, 0)\n",
        "        labels = torch.cat(labels, 0)\n",
        "        scores = torch.cat(scores, 0)\n",
        "        return boxes, labels, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXqT8_FYzB7K"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjjOYTFCjSN-"
      },
      "source": [
        "The loss function in SSD consists of a L1 loss between the ground truth bounding boxes and the predicted bounding boxes plus the cross entropy loss of the predictions in the ground truth bounding boxes and the predicted bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQ6n87Gy3-u"
      },
      "source": [
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/loss/ssd_loss.py\n",
        "\n",
        "class SSDLoss(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SSDLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def _hard_negative_mining(self, cls_loss, pos):\n",
        "        '''Return negative indices that is 3x the number as postive indices.\n",
        "        Args:\n",
        "          cls_loss: (tensor) cross entroy loss between cls_preds and cls_targets, sized [N,#anchors].\n",
        "          pos: (tensor) positive class mask, sized [N,#anchors].\n",
        "        Return:\n",
        "          (tensor) negative indices, sized [N,#anchors].\n",
        "        '''\n",
        "        cls_loss = cls_loss * (pos.float() - 1)\n",
        "\n",
        "        _, idx = cls_loss.sort(1)  # sort by negative losses\n",
        "        _, rank = idx.sort(1)      # [N,#anchors]\n",
        "\n",
        "        num_neg = 3*pos.sum(1)  # [N,]\n",
        "        neg = rank < num_neg[:,None]   # [N,#anchors]\n",
        "        return neg\n",
        "\n",
        "    def forward(self, loc_preds, loc_targets, cls_preds, cls_targets):\n",
        "        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
        "        Args:\n",
        "          loc_preds: (tensor) predicted locations, sized [N, #anchors, 4].\n",
        "          loc_targets: (tensor) encoded target locations, sized [N, #anchors, 4].\n",
        "          cls_preds: (tensor) predicted class confidences, sized [N, #anchors, #classes].\n",
        "          cls_targets: (tensor) encoded target labels, sized [N, #anchors].\n",
        "        loss:\n",
        "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + CrossEntropyLoss(cls_preds, cls_targets).\n",
        "        '''\n",
        "        pos = cls_targets > 0  # [N,#anchors]\n",
        "        batch_size = pos.size(0)\n",
        "        num_pos = pos.sum().item()\n",
        "\n",
        "        #===============================================================\n",
        "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
        "        #===============================================================\n",
        "        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
        "        loc_loss = F.smooth_l1_loss(loc_preds[mask], loc_targets[mask], reduction='sum')\n",
        "\n",
        "        #===============================================================\n",
        "        # cls_loss = CrossEntropyLoss(cls_preds, cls_targets)\n",
        "        #===============================================================\n",
        "        cls_loss = F.cross_entropy(cls_preds.view(-1,self.num_classes), \\\n",
        "                                   cls_targets.view(-1), reduction='none')  # [N*#anchors,]\n",
        "        cls_loss = cls_loss.view(batch_size, -1)\n",
        "        cls_loss[cls_targets<0] = 0  # set ignored loss to 0\n",
        "        neg = self._hard_negative_mining(cls_loss, pos)  # [N,#anchors]\n",
        "        cls_loss = cls_loss[pos|neg].sum()\n",
        "\n",
        "        #print('loc_loss: %.3f | cls_loss: %.3f' % (loc_loss.item()/num_pos, cls_loss.item()/num_pos), end=' | ')\n",
        "        loss = (loc_loss+cls_loss)/num_pos\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la0GalOUzMTy"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZpD4mPKjvOw"
      },
      "source": [
        "The SSD model itself is built on a convolutional neural network. Additional convolutional layers are added and the features which predict each bounding box are extracted from the later layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvALfd37zL25"
      },
      "source": [
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/models/ssd/net.py\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layers = self._make_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.layers(x)\n",
        "        return y\n",
        "\n",
        "    def _make_layers(self):\n",
        "        '''VGG16 layers.'''\n",
        "        cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.ReLU(True)]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class L2Norm(nn.Module):\n",
        "    '''L2Norm layer across all channels.'''\n",
        "    def __init__(self, in_features, scale):\n",
        "        super(L2Norm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(in_features))\n",
        "        self.reset_parameters(scale)\n",
        "\n",
        "    def reset_parameters(self, scale):\n",
        "        nn.init.constant(self.weight, scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.normalize(x, dim=1)\n",
        "        scale = self.weight[None,:,None,None]\n",
        "        return scale * x\n",
        "\n",
        "\n",
        "class VGG16Extractor300(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16Extractor300, self).__init__()\n",
        "\n",
        "        self.features = VGG16()\n",
        "        self.norm4 = L2Norm(512, 20)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "\n",
        "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1)\n",
        "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
        "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "\n",
        "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
        "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hs = []\n",
        "        h = self.features(x)\n",
        "        hs.append(self.norm4(h))  # conv4_3\n",
        "\n",
        "        h = F.max_pool2d(h, kernel_size=2, stride=2, ceil_mode=True)\n",
        "\n",
        "        h = F.relu(self.conv5_1(h))\n",
        "        h = F.relu(self.conv5_2(h))\n",
        "        h = F.relu(self.conv5_3(h))\n",
        "        h = F.max_pool2d(h, kernel_size=3, stride=1, padding=1, ceil_mode=True)\n",
        "\n",
        "        h = F.relu(self.conv6(h))\n",
        "        h = F.relu(self.conv7(h))\n",
        "        hs.append(h)  # conv7\n",
        "\n",
        "        h = F.relu(self.conv8_1(h))\n",
        "        h = F.relu(self.conv8_2(h))\n",
        "        hs.append(h)  # conv8_2\n",
        "\n",
        "        h = F.relu(self.conv9_1(h))\n",
        "        h = F.relu(self.conv9_2(h))\n",
        "        hs.append(h)  # conv9_2\n",
        "\n",
        "        h = F.relu(self.conv10_1(h))\n",
        "        h = F.relu(self.conv10_2(h))\n",
        "        hs.append(h)  # conv10_2\n",
        "\n",
        "        h = F.relu(self.conv11_1(h))\n",
        "        h = F.relu(self.conv11_2(h))\n",
        "        hs.append(h)  # conv11_2\n",
        "        return hs\n",
        "\n",
        "\n",
        "class SSD300(nn.Module):\n",
        "    steps = (8, 16, 32, 64, 100, 300)\n",
        "    box_sizes = (30, 60, 111, 162, 213, 264, 315)  # default bounding box sizes for each feature map.\n",
        "    aspect_ratios = ((2,), (2,3), (2,3), (2,3), (2,), (2,))\n",
        "    fm_sizes = (38, 19, 10, 5, 3, 1)\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(SSD300, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchors = (4, 6, 6, 6, 4, 4)\n",
        "        self.in_channels = (512, 1024, 512, 256, 256, 256)\n",
        "\n",
        "        self.extractor = VGG16Extractor300()\n",
        "        self.loc_layers = nn.ModuleList()\n",
        "        self.cls_layers = nn.ModuleList()\n",
        "        for i in range(len(self.in_channels)):\n",
        "        \tself.loc_layers += [nn.Conv2d(self.in_channels[i], self.num_anchors[i]*4, kernel_size=3, padding=1)]\n",
        "        \tself.cls_layers += [nn.Conv2d(self.in_channels[i], self.num_anchors[i]*self.num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        loc_preds = []\n",
        "        cls_preds = []\n",
        "        xs = self.extractor(x)\n",
        "        for i, x in enumerate(xs):\n",
        "            loc_pred = self.loc_layers[i](x)\n",
        "            loc_pred = loc_pred.permute(0,2,3,1).contiguous()\n",
        "            loc_preds.append(loc_pred.view(loc_pred.size(0),-1,4))\n",
        "\n",
        "            cls_pred = self.cls_layers[i](x)\n",
        "            cls_pred = cls_pred.permute(0,2,3,1).contiguous()\n",
        "            cls_preds.append(cls_pred.view(cls_pred.size(0),-1,self.num_classes))\n",
        "\n",
        "        loc_preds = torch.cat(loc_preds, 1)\n",
        "        cls_preds = torch.cat(cls_preds, 1)\n",
        "        return loc_preds, cls_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEBxs1JRTjro"
      },
      "source": [
        "### Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4ruwvh44rt_"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 5 # 0 corresponds to no extreme weather event\n",
        "model = SSD300(num_classes)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UamZHQbqT1go"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = SSDLoss(num_classes)\n",
        "box_coder = SSDBoxCoder(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ock9sG1EkIIy"
      },
      "source": [
        "We use box_coder to convert between the representation of bounding boxes by their coordinates and the representation of bounding boxes in the higher dimensional space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_02pSF2ezWqk"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbIitPrzzaKu"
      },
      "source": [
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/transforms/random_distort.py\n",
        "def random_distort(\n",
        "    img,\n",
        "    brightness_delta=32/255.,\n",
        "    contrast_delta=0.5,\n",
        "    saturation_delta=0.5,\n",
        "    hue_delta=0.1):\n",
        "    '''A color related data augmentation used in SSD.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be color augmented.\n",
        "      brightness_delta: (float) shift of brightness, range from [1-delta,1+delta].\n",
        "      contrast_delta: (float) shift of contrast, range from [1-delta,1+delta].\n",
        "      saturation_delta: (float) shift of saturation, range from [1-delta,1+delta].\n",
        "      hue_delta: (float) shift of hue, range from [-delta,delta].\n",
        "    Returns:\n",
        "      img: (PIL.Image) color augmented image.\n",
        "    '''\n",
        "    def brightness(img, delta):\n",
        "        if random.random() < 0.5:\n",
        "            img = transforms.ColorJitter(brightness=delta)(img)\n",
        "        return img\n",
        "\n",
        "    def contrast(img, delta):\n",
        "        if random.random() < 0.5:\n",
        "            img = transforms.ColorJitter(contrast=delta)(img)\n",
        "        return img\n",
        "\n",
        "    def saturation(img, delta):\n",
        "        if random.random() < 0.5:\n",
        "            img = transforms.ColorJitter(saturation=delta)(img)\n",
        "        return img\n",
        "\n",
        "    def hue(img, delta):\n",
        "        if random.random() < 0.5:\n",
        "            img = transforms.ColorJitter(hue=delta)(img)\n",
        "        return img\n",
        "\n",
        "    img = brightness(img, brightness_delta)\n",
        "    if random.random() < 0.5:\n",
        "        img = contrast(img, contrast_delta)\n",
        "        img = saturation(img, saturation_delta)\n",
        "        img = hue(img, hue_delta)\n",
        "    else:\n",
        "        img = saturation(img, saturation_delta)\n",
        "        img = hue(img, hue_delta)\n",
        "        img = contrast(img, contrast_delta)\n",
        "    return img\n",
        "\n",
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/transforms/random_paste.py\n",
        "def random_paste(img, boxes, max_ratio=4, fill=0):\n",
        "    '''Randomly paste the input image on a larger canvas.\n",
        "    If boxes is not None, adjust boxes accordingly.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be flipped.\n",
        "      boxes: (tensor) object boxes, sized [#obj,4].\n",
        "      max_ratio: (int) maximum ratio of expansion.\n",
        "      fill: (tuple) the RGB value to fill the canvas.\n",
        "    Returns:\n",
        "      canvas: (PIL.Image) canvas with image pasted.\n",
        "      boxes: (tensor) adjusted object boxes.\n",
        "    '''\n",
        "    w, h = img.size\n",
        "    ratio = random.uniform(1, max_ratio)\n",
        "    ow, oh = int(w*ratio), int(h*ratio)\n",
        "    canvas = Image.new('RGB', (ow,oh), fill)\n",
        "\n",
        "    x = random.randint(0, ow - w)\n",
        "    y = random.randint(0, oh - h)\n",
        "    canvas.paste(img, (x,y))\n",
        "\n",
        "    if boxes is not None:\n",
        "        boxes = boxes + torch.tensor([x,y,x,y], dtype=torch.float)\n",
        "    return canvas, boxes\n",
        "\n",
        "\n",
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/transforms/resize.py\n",
        "def resize(img, boxes, size, max_size=1000, random_interpolation=False):\n",
        "    '''Resize the input PIL image to given size.\n",
        "    If boxes is not None, resize boxes accordingly.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be resized.\n",
        "      boxes: (tensor) object boxes, sized [#obj,4].\n",
        "      size: (tuple or int)\n",
        "        - if is tuple, resize image to the size.\n",
        "        - if is int, resize the shorter side to the size while maintaining the aspect ratio.\n",
        "      max_size: (int) when size is int, limit the image longer size to max_size.\n",
        "                This is essential to limit the usage of GPU memory.\n",
        "      random_interpolation: (bool) randomly choose a resize interpolation method.\n",
        "    Returns:\n",
        "      img: (PIL.Image) resized image.\n",
        "      boxes: (tensor) resized boxes.\n",
        "    Example:\n",
        "    >> img, boxes = resize(img, boxes, 600)  # resize shorter side to 600\n",
        "    >> img, boxes = resize(img, boxes, (500,600))  # resize image size to (500,600)\n",
        "    >> img, _ = resize(img, None, (500,600))  # resize image only\n",
        "    '''\n",
        "    w, h = img.size\n",
        "    if isinstance(size, int):\n",
        "        size_min = min(w,h)\n",
        "        size_max = max(w,h)\n",
        "        sw = sh = float(size) / size_min\n",
        "        if sw * size_max > max_size:\n",
        "            sw = sh = float(max_size) / size_max\n",
        "        ow = int(w * sw + 0.5)\n",
        "        oh = int(h * sh + 0.5)\n",
        "    else:\n",
        "        ow, oh = size\n",
        "        sw = float(ow) / w\n",
        "        sh = float(oh) / h\n",
        "\n",
        "    method = random.choice([\n",
        "        Image.BOX,\n",
        "        Image.NEAREST,\n",
        "        Image.HAMMING,\n",
        "        Image.BICUBIC,\n",
        "        Image.LANCZOS,\n",
        "        Image.BILINEAR]) if random_interpolation else Image.BILINEAR\n",
        "    img = img.resize((ow,oh), method)\n",
        "    if boxes is not None:\n",
        "        boxes = boxes * torch.tensor([sw,sh,sw,sh])\n",
        "    return img, boxes\n",
        "\n",
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/transforms/random_flip.py\n",
        "def random_flip(img, boxes):\n",
        "    '''Randomly flip PIL image.\n",
        "    If boxes is not None, flip boxes accordingly.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be flipped.\n",
        "      boxes: (tensor) object boxes, sized [#obj,4].\n",
        "    Returns:\n",
        "      img: (PIL.Image) randomly flipped image.\n",
        "      boxes: (tensor) randomly flipped boxes.\n",
        "    '''\n",
        "    if random.random() < 0.5:\n",
        "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        w = img.width\n",
        "        if boxes is not None:\n",
        "            xmin = w - boxes[:,2]\n",
        "            xmax = w - boxes[:,0]\n",
        "            boxes[:,0] = xmin\n",
        "            boxes[:,2] = xmax\n",
        "    return img, boxes\n",
        "\n",
        "# https://github.com/kuangliu/torchcv/blob/master/torchcv/transforms/random_crop.py\n",
        "def random_crop(\n",
        "        img, boxes, labels,\n",
        "        min_scale=0.3,\n",
        "        max_aspect_ratio=2.):\n",
        "    '''Randomly crop a PIL image.\n",
        "    Args:\n",
        "      img: (PIL.Image) image.\n",
        "      boxes: (tensor) bounding boxes, sized [#obj, 4].\n",
        "      labels: (tensor) bounding box labels, sized [#obj,].\n",
        "      min_scale: (float) minimal image width/height scale.\n",
        "      max_aspect_ratio: (float) maximum width/height aspect ratio.\n",
        "    Returns:\n",
        "      img: (PIL.Image) cropped image.\n",
        "      boxes: (tensor) object boxes.\n",
        "      labels: (tensor) object labels.\n",
        "    '''\n",
        "    imw, imh = img.size\n",
        "    params = [(0, 0, imw, imh)]  # crop roi (x,y,w,h) out\n",
        "    for min_iou in (0, 0.1, 0.3, 0.5, 0.7, 0.9):\n",
        "        for _ in range(100):\n",
        "            scale = random.uniform(min_scale, 1)\n",
        "            aspect_ratio = random.uniform(\n",
        "                max(1/max_aspect_ratio, scale*scale),\n",
        "                min(max_aspect_ratio, 1/(scale*scale)))\n",
        "            w = int(imw * scale * math.sqrt(aspect_ratio))\n",
        "            h = int(imh * scale / math.sqrt(aspect_ratio))\n",
        "\n",
        "            x = random.randrange(imw - w)\n",
        "            y = random.randrange(imh - h)\n",
        "\n",
        "            roi = torch.tensor([[x,y,x+w,y+h]], dtype=torch.float)\n",
        "            ious = box_iou(boxes, roi)\n",
        "            if ious.min() >= min_iou:\n",
        "                params.append((x,y,w,h))\n",
        "                break\n",
        "\n",
        "    x,y,w,h = random.choice(params)\n",
        "    img = img.crop((x,y,x+w,y+h))\n",
        "\n",
        "    center = (boxes[:,:2] + boxes[:,2:]) / 2\n",
        "    mask = (center[:,0]>=x) & (center[:,0]<=x+w) \\\n",
        "         & (center[:,1]>=y) & (center[:,1]<=y+h)\n",
        "    if mask.any():\n",
        "        boxes = boxes[mask] - torch.tensor([x,y,x,y], dtype=torch.float)\n",
        "        boxes = box_clamp(boxes,0,0,w,h)\n",
        "        labels = labels[mask]\n",
        "    else:\n",
        "        boxes = torch.tensor([[0,0,0,0]], dtype=torch.float)\n",
        "        labels = torch.tensor([0], dtype=torch.long)\n",
        "    return img, boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhWQejRB72ji"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9-teF1Ekb_p"
      },
      "source": [
        "For each of train, valid, and test, we assume there is a folder that contains a list of lists of bounding boxes in bboxes.txt, a list of labels corresponding to the bounding boxes in labels.txt, and 300x300 images with three channels labeled as i.jpg for $i \\in [0,N-1]$ where $N$ is the number of images in the folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXuT29YR77SV"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Modified from https://github.com/kuangliu/torchcv/blob/master/examples/ssd/train.py\n",
        "def transform_train(img, boxes, labels):\n",
        "    img_size=img.shape[-1]\n",
        "    img = transforms.ToPILImage()(img)\n",
        "    img = random_distort(img)\n",
        "    if random.random() < 0.5:\n",
        "        img, boxes = random_paste(img, boxes, max_ratio=4, fill=(123,116,103))\n",
        "    img, boxes, labels = random_crop(img, boxes, labels)\n",
        "    img, boxes = resize(img, boxes, size=(img_size,img_size), random_interpolation=True)\n",
        "    img, boxes = random_flip(img, boxes)\n",
        "    img = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
        "    ])(img)\n",
        "    return img, boxes, labels\n",
        "\n",
        "# Modified from https://github.com/kuangliu/torchcv/blob/master/examples/ssd/train.py\n",
        "def transform_test(img, boxes, labels):\n",
        "    img_size = img.shape[-1]\n",
        "    img = transforms.ToPILImage()(img)\n",
        "    img, boxes = resize(img, boxes, size=(img_size,img_size))\n",
        "    img = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
        "    ])(img)\n",
        "    return img, boxes, labels\n",
        "\n",
        "# Modified from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "class ExtremeWeatherDataset(Dataset):\n",
        "  def __init__(self, dir, transform_fn=None):\n",
        "    self.transform_fn = transform_fn\n",
        "    self.bboxes = []\n",
        "    self.labels = []\n",
        "    with open(os.path.join(dir, 'bboxes.txt')) as f:\n",
        "      for line in f:\n",
        "        self.bboxes += [torch.Tensor(eval(line))]\n",
        "    with open(os.path.join(dir, 'labels.txt')) as f:\n",
        "      for line in f:\n",
        "        self.labels += [torch.Tensor(eval(line))]\n",
        "    self.dir = dir\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.bboxes)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.dir, str(idx)+'.jpg')\n",
        "    image = read_image(img_path)/255 # scale pixel values between 0 and 1\n",
        "    bbox = self.bboxes[idx] \n",
        "    label = self.labels[idx]\n",
        "    if self.transform_fn != None:\n",
        "      return self.transform_fn(image, bbox, label)\n",
        "    return [image, bbox, label] # scale coordinates between 0 and 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY1VTDCWIbZR"
      },
      "source": [
        "prefix = '/content/drive/My Drive/extremeweather/' # where the data is stored\n",
        "train_data = ExtremeWeatherDataset(dir=prefix+'train/', transform_fn = transform_train)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "valid_data = ExtremeWeatherDataset(dir=prefix+'valid/', transform_fn = transform_test)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "#test_data = ExtremeWeatherDataset(dir=prefix+'test/', transform_fn = transform_test)\n",
        "#test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCQ_TE8SRugI"
      },
      "source": [
        "### Visualizing Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnyU760glKhL"
      },
      "source": [
        "Before training, we show the images and bounding boxes we are working with. We chose 3 of the 16 channels by hand for explainability of the extreme weather events (i.e. Pressure, Water, Wind)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5irmhuDtRxi_"
      },
      "source": [
        "EVENTS = ['Tropical Depression', 'Tropical Cyclone', 'Extratropical Cyclone', 'Atmospheric River']\n",
        "NAMES = ['Sea Level Pressure', 'Total Water', 'Lowest Zonal Wind']\n",
        "COLORS = ['red', 'blue', 'white', 'black']\n",
        "\n",
        "def plot_image(image, box=None, labels=None, img_size=1):\n",
        "  if len(image.shape) == 2: image = np.array([image])\n",
        "  # Adapt subplot to the number of channels\n",
        "  num_channels = image.shape[0]\n",
        "  num_rows = int(np.sqrt(num_channels))\n",
        "  num_cols = int(np.round(num_channels//num_rows))\n",
        "  fig, axs = plt.subplots(num_rows,num_cols, figsize=(15,15)) \n",
        "  fig.tight_layout()\n",
        "  for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "      channel = i*num_cols + j \n",
        "      if channel < image.shape[0]:\n",
        "        if num_channels == 1: ax = axs\n",
        "        elif num_rows == 1: ax = axs[j]\n",
        "        elif num_rows > 1: ax = axs[i,j]\n",
        "        ax.axis('off')\n",
        "        ax.imshow(image[channel,]) \n",
        "        if box != None:\n",
        "          addbox(ax, box, labels, img_size)\n",
        "        ax.set_title(NAMES[channel])\n",
        "  if box != None:\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "  plt.show()\n",
        "    \n",
        "def addbox(ax, box, labels, img_size):\n",
        "  for i in range(len(box)):\n",
        "    left, bottom, right, top = box[i]*img_size\n",
        "    handles, existing_labels = ax.get_legend_handles_labels()\n",
        "    label = \"\" if (labels == None) or (EVENTS[int(labels[i])] in existing_labels) else EVENTS[int(labels[i])]\n",
        "    ax.add_patch(patches.Rectangle(xy=(left, bottom), width=right-left,\n",
        "                                    height=top-bottom, fill=False,\n",
        "                                    label=label, color=COLORS[int(labels[i])]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGS66Av8Spdy"
      },
      "source": [
        "# Plot images\n",
        "images, bboxes, labels = next(iter(valid_dataloader))\n",
        "idx = 0\n",
        "image, box, label = images[idx], bboxes[idx], labels[idx]\n",
        "plot_image(image, box, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy7ziTYf6Kbl"
      },
      "source": [
        "## Plot augmented images\n",
        "image, box, label = transform_train(image, box, label)\n",
        "plot_image(image, box, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0No9ngWlcKP"
      },
      "source": [
        "Since the original images are relatively homogeneous, it's important that we augment the data so the model learns the properties of the extreme weather events rather than the structure of the weather patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV2oYcLXx2Ex"
      },
      "source": [
        "### Training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoi3-cvJ1gGA"
      },
      "source": [
        "### Helper functions\n",
        "def clear_ram():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKhnNn_o0G1l"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, box_coder):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for images, bboxes, labels in iterator:\n",
        "    images = torch.stack(images).to(device)\n",
        "    loc_targets, cls_targets = zip(*[box_coder.encode(bbox, label) for bbox, label in zip(bboxes, labels)])\n",
        "    loc_targets = torch.stack(loc_targets).long().to(device)\n",
        "    cls_targets = torch.stack(cls_targets).long().to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loc_preds, cls_preds = model(images)\n",
        "    loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    del loc_preds, cls_preds\n",
        "    clear_ram()    \n",
        "  \n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNnRf9aR1PzU"
      },
      "source": [
        "def evaluate(model, iterator, criterion, box_coder):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, bboxes, labels in iterator:\n",
        "      images = torch.stack(images).to(device)\n",
        "      loc_targets, cls_targets = zip(*[box_coder.encode(bbox, label) for bbox, label in zip(bboxes, labels)])\n",
        "      loc_targets = torch.stack(loc_targets).long().to(device)\n",
        "      cls_targets = torch.stack(cls_targets).long().to(device)\n",
        "  \n",
        "      loc_preds, cls_preds = model(images)\n",
        "      loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      del loc_preds, cls_preds\n",
        "      clear_ram()\n",
        "    \n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vho56r35tVS"
      },
      "source": [
        "if os.path.exists(prefix+'losses.txt'):\n",
        "  with open(prefix+'losses.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    train_loss, valid_loss = eval(lines[0]), eval(lines[1])\n",
        "    best_loss = min(valid_loss)\n",
        "else:\n",
        "  train_loss, valid_loss = [], []\n",
        "  best_loss = float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBU6op60pLPS"
      },
      "source": [
        "if os.path.exists(prefix+'model.pt'):\n",
        "  model.load_state_dict(torch.load(prefix+'model.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NnKTthx1efW"
      },
      "source": [
        "n_epochs = 5\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  start = time.time()\n",
        "  train_loss += [train(model, train_dataloader, optimizer, criterion, box_coder)]\n",
        "  valid_loss += [evaluate(model, valid_dataloader, criterion, box_coder)]\n",
        "\n",
        "  epoch_mins, epoch_secs = epoch_time(start, time.time())\n",
        "\n",
        "  if valid_loss[-1] < best_loss:\n",
        "    best_loss = valid_loss[-1]\n",
        "    torch.save(model.state_dict(), prefix+'model.pt')\n",
        "\n",
        "  print(f'\\nEpoch #{epoch+1:02} Time: {epoch_mins}m {epoch_secs}s', end =' ')\n",
        "  print(f'| Train Loss: {train_loss[-1]:.3f}', end=' ')\n",
        "  print(f'| Valid Loss: {valid_loss[-1]:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esMaDFuj26Uc"
      },
      "source": [
        "model.load_state_dict(torch.load(prefix+'model.pt'))\n",
        "#test_loss = evaluate(model, test_dataloader, criterion)\n",
        "#print(f'\\t Test Loss: {test_loss:.3f}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2i21Va66Zal"
      },
      "source": [
        "images, bboxes, labels = next(iter(train_dataloader))\n",
        "with torch.no_grad():\n",
        "  images = torch.stack(images).to(device) \n",
        "  loc_targets, cls_targets = zip(*[box_coder.encode(bbox, label) for bbox, label in zip(bboxes, labels)])\n",
        "  loc_preds, cls_preds = model(images)\n",
        "  loc_preds = loc_preds.cpu()\n",
        "  cls_preds = cls_preds.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-RCe-Z8607q"
      },
      "source": [
        "img_idx = 7\n",
        "image = images[img_idx].cpu()\n",
        "plot_image(image, bboxes[img_idx], labels[img_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpKaRSzPp0wM"
      },
      "source": [
        "pred_boxes, pred_labels, pred_scores = box_coder.decode(loc_preds[img_idx], cls_preds[img_idx], score_thresh=.65)\n",
        "plot_image(image, pred_boxes, pred_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CRC0Pqxp1o3"
      },
      "source": [
        "pred_boxes, pred_labels, pred_scores = box_coder.decode(loc_preds[img_idx], cls_preds[img_idx], score_thresh=.5)\n",
        "plot_image(image, pred_boxes, pred_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}