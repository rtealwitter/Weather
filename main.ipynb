{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlhhRswGx2Es"
      },
      "source": [
        "## Single Shot Detector for ExtremeWeather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXRh1Oix2Ew"
      },
      "source": [
        "### Preprocessing the Data\n",
        "The [ExtremeWeather](https://extremeweatherdataset.github.io/) data set consists of weather data for a specific 25km region from 1979 to 2005. \n",
        "The data is organized by year and contains 4 images per day. Each image has 768x1152 pixels across 16 channels for different weather variables. \n",
        "In addition, each day has up to 15 bounding boxes\n",
        "surrounding extreme weather events classified as Tropical Depression, Tropical Cyclone, Extratropical Cyclone, and Atmospheric River.\n",
        "\n",
        "The goal is to correctly identify and classify extreme weather events. \n",
        "The first challenge is the sheer size of the data set.\n",
        "Each year, even when compressed, takes 64 GB to store.\n",
        "As the \"small\" data set, we consider training on 1979 and 1981 then testing on 1984.\n",
        "We preprocess each year by considering only the month of July and rescaling each image so its of size 300x300.\n",
        "In addition, we choose 3 of the 16 channels by hand for 'visual explainability' of bounding boxes.\n",
        "All together, we are able to reduce the size of the compressed data from 64 GB to 40 MB consisting of 124 300x300 images and bounding boxes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHqa8ulBK3OI"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leBCbKBjK1PD",
        "outputId": "0664b234-93e1-4dfd-bf40-e080d131dd45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Set up\n",
        "### Loading Data\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "### Model\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "### Visualization\n",
        "from matplotlib import patches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### Store data in drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhWQejRB72ji"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXuT29YR77SV"
      },
      "source": [
        "# Modified from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "class ExtremeWeatherDataset(Dataset):\n",
        "  def __init__(self, bbox_file, img_dir):\n",
        "    self.bboxes = []\n",
        "    with open(bbox_file, 'r') as f:\n",
        "      for line in f:\n",
        "        self.bboxes += [torch.tensor(eval(line))]\n",
        "    self.img_dir = img_dir\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.bboxes)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, str(idx)+'.jpg')\n",
        "    image = read_image(img_path)\n",
        "    bbox = self.bboxes[idx]\n",
        "    return torch.tensor(image), bbox"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY1VTDCWIbZR"
      },
      "source": [
        "prefix = '/content/drive/My Drive/extremeweather/' # where the data is stored\n",
        "train_data = ExtremeWeatherDataset(bbox_file=prefix+'train/bboxes.txt', img_dir=prefix+'train')\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnqxWJxUKSnS",
        "outputId": "ceb86510-1e4f-4e87-ff3e-5d349757fb42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "images, bboxes = next(iter(train_dataloader))\n",
        "print(images.shape)\n",
        "print(bboxes.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 300, 300])\n",
            "torch.Size([64, 15, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCQ_TE8SRugI"
      },
      "source": [
        "### Visualizing Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5irmhuDtRxi_"
      },
      "source": [
        "def plot_image(image, box):\n",
        "    plt.rcParams.update({'font.size': 7})\n",
        "    if len(image.shape) == 2: image = np.array([image])\n",
        "    num_channels = image.shape[0]\n",
        "    num_rows = int(np.sqrt(num_channels))\n",
        "    num_cols = int(np.round(num_channels//num_rows))\n",
        "    fig, axs = plt.subplots(num_rows,num_cols) \n",
        "    fig.tight_layout()\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            channel = i*num_cols + j \n",
        "            if channel < image.shape[0]:\n",
        "                if num_channels == 1: ax = axs\n",
        "                elif num_rows == 1: ax = axs[j]\n",
        "                elif num_rows > 1: ax = axs[i,j]\n",
        "                ax.axis('off')\n",
        "                ax.imshow(image[channel,])                \n",
        "                addbox(ax, box)\n",
        "                ax.set_title(NAMES[channel])\n",
        "    plt.show()\n",
        "    \n",
        "def addbox(ax, box):\n",
        "    for row in box:\n",
        "        if not np.all(row == -1):\n",
        "            ymin, xmin, ymax, xmax, event_class = row\n",
        "            ax.add_patch(patches.Rectangle(xy=(xmin, ymin),width=xmax-xmin, height=ymax-ymin, fill=False, label=EVENTS[event_class]))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV2oYcLXx2Ex"
      },
      "source": [
        "### Training the SSD Model\n",
        "Instead of building the SSD model from scratch, we use a pretrained model on the COCO database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3A7BoQvx2Ez"
      },
      "source": [
        "gc.collect()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\n",
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuMLngwx2Ez"
      },
      "source": [
        "from preprocess import read_data\n",
        "year = '1979'\n",
        "images, boxes = read_data(year)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}